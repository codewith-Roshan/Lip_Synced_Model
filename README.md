# Lip_Synced_Model
Creating a Lip Synced video with the help of the video and audio links provided in the pdf above Following is the link for the Pre- Trained model required in the first assignment:Â https://github.com/Rudrabha/Wav2Lip


Running the Wav2Lip Model:
1.Clone the Repository:
Start by cloning the Wav2Lip repository from GitHub. The official repository can be found here: Wav2Lip.


2. Install Dependencies:
Ensure you have all the required dependencies installed. Commonly used tools are ffmpeg and librosa. You can install Python dependencies using:


3.Download Pre-trained Model:
Download the pre-trained Wav2Lip model weights. The model checkpoint file is usually large and hosted on external platforms. Follow the instructions in the README or documentation to obtain the pre-trained model.


4.Run Inference:
Use the model to generate lip-synced videos by providing an input video and an audio file.
Replace path/to/pretrained_model.pth, path/to/input_video.mp4, and path/to/audio.wav with the actual paths.


5.Evaluating Performance:
Evaluating the performance of a lip-synced AI model can involve both subjective and objective metrics. Here are some suggestions:

.....Visual Inspection:....
Watch the generated videos and visually inspect the lip-syncing. Check if the lip movements match the audio.

.......Lip Sync Accuracy:.....
Manually analyze specific points in the video to see how well the lip movements align with the audio. You can use tools like video editing software to step through frames.
Word-Level Accuracy:


Explore metrics used in the field of audio-visual signal processing, such as correlation coefficients between lip movements and audio signals.
